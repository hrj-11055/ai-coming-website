#!/usr/bin/env python3
"""
Claude Code Conversation Analyzer
Generates personal coding diaries from conversation history
"""

import json
import os
import re
import sys
from datetime import datetime, timedelta
from pathlib import Path
from collections import defaultdict

# Configuration
HISTORY_FILE = Path.home() / '.claude' / 'history.jsonl'
DIARY_BASE_DIR = Path.home() / 'æ—¥è®°'

# Work type classification keywords
KEYWORDS = {
    'bug_fix': ['ä¿®å¤', 'fix', 'bug', 'é”™è¯¯', 'æŠ¥é”™', 'è§£å†³', 'error', 'issue', 'é—®é¢˜'],
    'optimization': ['ä¼˜åŒ–', 'æ”¹è¿›', 'æå‡', 'improve', 'optimize', 'refactor', 'é‡æ„'],
    'new_feature': ['æ–°å¢', 'æ·»åŠ ', 'create', 'add', 'implement', 'åŠŸèƒ½', 'feature'],
    'documentation': ['æ–‡æ¡£', 'readme', 'è¯´æ˜', 'document', 'write', 'æ›´æ–°'],
    'testing': ['æµ‹è¯•', 'test', 'è¿è¡Œ', 'run', 'éªŒè¯'],
    'deployment': ['éƒ¨ç½²', 'deploy', 'å‘å¸ƒ', 'ä¸Šçº¿'],
    'configuration': ['é…ç½®', 'config', 'ç¯å¢ƒ', 'è®¾ç½®', 'setup'],
}

WORK_TYPE_EMOJIS = {
    'bug_fix': 'ğŸ”§',
    'optimization': 'âš¡',
    'new_feature': 'âœ¨',
    'documentation': 'ğŸ“',
    'testing': 'ğŸ§ª',
    'deployment': 'ğŸš€',
    'configuration': 'âš™ï¸',
    'other': 'ğŸ“¦'
}

WORK_TYPE_NAMES = {
    'bug_fix': 'Bugä¿®å¤',
    'optimization': 'æ€§èƒ½ä¼˜åŒ–',
    'new_feature': 'æ–°åŠŸèƒ½',
    'documentation': 'æ–‡æ¡£æ›´æ–°',
    'testing': 'æµ‹è¯•å·¥ä½œ',
    'deployment': 'éƒ¨ç½²å‘å¸ƒ',
    'configuration': 'é…ç½®è°ƒæ•´',
    'other': 'å…¶ä»–'
}


def read_history(date_filter=None):
    """Read conversation history from JSONL file"""
    if not HISTORY_FILE.exists():
        print(f"âŒ å†å²è®°å½•æ–‡ä»¶ä¸å­˜åœ¨: {HISTORY_FILE}")
        return []

    conversations = []
    try:
        with open(HISTORY_FILE, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                try:
                    data = json.loads(line.strip())
                    if not data:
                        continue

                    # Convert timestamp from milliseconds to datetime
                    timestamp_ms = data.get('timestamp', 0)
                    if timestamp_ms:
                        timestamp = datetime.fromtimestamp(timestamp_ms / 1000)

                        # Apply date filter if specified
                        if date_filter:
                            if not (date_filter['start'] <= timestamp < date_filter['end']):
                                continue

                        conversations.append({
                            'timestamp': timestamp,
                            'project': data.get('project', 'unknown'),
                            'content': data.get('display', ''),
                            'session_id': data.get('sessionId', ''),
                            'line_num': line_num
                        })
                except json.JSONDecodeError as e:
                    print(f"âš ï¸  è·³è¿‡ç¬¬{line_num}è¡Œ: JSONè§£æé”™è¯¯")
                    continue
                except Exception as e:
                    print(f"âš ï¸  è·³è¿‡ç¬¬{line_num}è¡Œ: {e}")
                    continue

        print(f"âœ… è¯»å–äº† {len(conversations)} æ¡å¯¹è¯è®°å½•")
        return conversations

    except Exception as e:
        print(f"âŒ è¯»å–å†å²æ–‡ä»¶å¤±è´¥: {e}")
        return []


def classify_work(text):
    """Classify conversation by work type using keywords"""
    if not text:
        return 'other'

    text_lower = text.lower()

    for work_type, keywords in KEYWORDS.items():
        if any(keyword in text_lower for keyword in keywords):
            return work_type

    return 'other'


def get_project_name(project_path):
    """Extract friendly project name from path"""
    if project_path == 'unknown':
        return 'æœªçŸ¥é¡¹ç›®'

    # Get last component of path
    return Path(project_path).name


def analyze_conversations(conversations, project_filter=None):
    """Analyze and categorize conversations"""
    # Group by project and work type
    by_project = defaultdict(lambda: defaultdict(list))

    for conv in conversations:
        project = conv['project']
        project_name = get_project_name(project)

        # Apply project filter if specified
        if project_filter and project_filter.lower() not in project.lower():
            continue

        work_type = classify_work(conv['content'])

        by_project[project_name][work_type].append(conv)

    return by_project


def generate_markdown_diary(conversations, date_str):
    """Generate Markdown format diary entry"""
    if not conversations:
        return None

    # Analyze conversations
    by_project = analyze_conversations(conversations)

    # Calculate statistics
    total_conversations = len(conversations)
    unique_projects = len(by_project)
    most_active_project = max(by_project.items(), key=lambda x: sum(len(v) for v in x[1].values()))[0] if by_project else 'N/A'

    # Build markdown content
    md_lines = []

    # Header
    md_lines.append(f"# ç¼–ç¨‹æ—¥è®° - {date_str}")
    md_lines.append("")
    md_lines.append(f"## ğŸ“Š ä»Šæ—¥ç»Ÿè®¡")
    md_lines.append(f"- **å¯¹è¯æ¬¡æ•°**: {total_conversations}æ¬¡")
    md_lines.append(f"- **æ¶‰åŠé¡¹ç›®**: {unique_projects}ä¸ª")
    md_lines.append(f"- **ä¸»è¦é¡¹ç›®**: {most_active_project}")
    md_lines.append("")

    # Group by work type across all projects
    by_work_type = defaultdict(list)
    for project_name, work_types in by_project.items():
        for work_type, convs in work_types.items():
            for conv in convs:
                by_work_type[work_type].append({
                    'project': project_name,
                    'content': conv['content'],
                    'timestamp': conv['timestamp']
                })

    # Generate sections for each work type
    for work_type in ['bug_fix', 'optimization', 'new_feature', 'documentation', 'testing', 'deployment', 'configuration', 'other']:
        items = by_work_type.get(work_type, [])
        if not items:
            continue

        emoji = WORK_TYPE_EMOJIS.get(work_type, 'ğŸ“¦')
        type_name = WORK_TYPE_NAMES.get(work_type, work_type.capitalize())

        md_lines.append(f"## {emoji} {type_name} ({len(items)}ä¸ª)")
        md_lines.append("")

        for item in items:
            project = item['project']
            content = item['content']
            time_str = item['timestamp'].strftime('%H:%M')

            # Truncate long content
            if len(content) > 200:
                content = content[:200] + "..."

            md_lines.append(f"### {project}é¡¹ç›®")
            md_lines.append(f"**æ—¶é—´**: {time_str}")
            md_lines.append(f"**å†…å®¹**: {content}")
            md_lines.append("")
            md_lines.append("---")
            md_lines.append("")

    # Add insights section if there are enough conversations
    if total_conversations >= 5:
        md_lines.append("## ğŸ’¡ ä»Šæ—¥æ”¶è·")
        md_lines.append("")
        md_lines.append("*ï¼ˆæ‰‹åŠ¨æ·»åŠ ä»Šå¤©å­¦åˆ°çš„çŸ¥è¯†ç‚¹å’Œç»éªŒï¼‰*")
        md_lines.append("")
        md_lines.append("")

    # Add tomorrow plan section
    md_lines.append("## ğŸ“… æ˜æ—¥è®¡åˆ’")
    md_lines.append("")
    md_lines.append("- [ ] å¾…å®š")
    md_lines.append("")
    md_lines.append("")

    # Add important conversations section
    important_convs = [c for c in conversations if len(c['content']) > 50]
    if important_convs:
        md_lines.append("## ğŸ”— é‡è¦å¯¹è¯ç‰‡æ®µ")
        md_lines.append("")
        for conv in important_convs[:5]:  # Top 5
            content = conv['content'][:150]
            if len(conv['content']) > 150:
                content += "..."
            md_lines.append(f"- {content}")
        md_lines.append("")

    # Footer
    md_lines.append("---")
    md_lines.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    md_lines.append(f"**ä¿å­˜ä½ç½®**: {DIARY_BASE_DIR}")
    md_lines.append("")

    return "\n".join(md_lines)


def save_diary(markdown_content, date_str):
    """Save diary to local file"""
    # Parse date
    date_obj = datetime.strptime(date_str, '%Y-%m-%d')

    # Create directory structure: YYYY/MM/
    year_month_dir = DIARY_BASE_DIR / str(date_obj.year) / f"{date_obj.month:02d}"
    year_month_dir.mkdir(parents=True, exist_ok=True)

    # File path: YYYY/MM/YYYY-MM-DD.md
    filename = f"{date_str}.md"
    file_path = year_month_dir / filename

    # Check if file already exists
    if file_path.exists():
        print(f"âš ï¸  æ—¥è®°æ–‡ä»¶å·²å­˜åœ¨: {file_path}")
        response = input("æ˜¯å¦è¦†ç›–? (y/N): ").strip().lower()
        if response != 'y':
            print("âŒ å–æ¶ˆä¿å­˜")
            return None

    # Write content
    try:
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)

        print(f"âœ… æ—¥è®°å·²ä¿å­˜: {file_path}")
        return file_path

    except Exception as e:
        print(f"âŒ ä¿å­˜å¤±è´¥: {e}")
        return None


def main():
    """Main function"""
    import argparse

    parser = argparse.ArgumentParser(description='ç”Ÿæˆç¼–ç¨‹æ—¥è®°')
    parser.add_argument('--date', default=datetime.now().strftime('%Y-%m-%d'),
                        help='æ—¥æœŸ (YYYY-MM-DD), é»˜è®¤ä»Šå¤©')
    parser.add_argument('--project', help='åªåˆ†ææŒ‡å®šé¡¹ç›®')
    parser.add_argument('--output', help='è‡ªå®šä¹‰ä¿å­˜è·¯å¾„')
    parser.add_argument('--print', action='store_true', help='æ‰“å°åˆ°ç»ˆç«¯')

    args = parser.parse_args()

    # Parse date
    try:
        date_obj = datetime.strptime(args.date, '%Y-%m-%d')
    except ValueError:
        print(f"âŒ æ— æ•ˆçš„æ—¥æœŸæ ¼å¼: {args.date}")
        sys.exit(1)

    # Date filter
    date_filter = {
        'start': date_obj.replace(hour=0, minute=0, second=0, microsecond=0),
        'end': date_obj.replace(hour=23, minute=59, second=59, microsecond=999999)
    }

    print(f"ğŸ“– æ­£åœ¨è¯»å– {args.date} çš„å¯¹è¯è®°å½•...")
    conversations = read_history(date_filter)

    if not conversations:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ° {args.date} çš„å¯¹è¯è®°å½•")
        sys.exit(1)

    # Apply project filter
    if args.project:
        print(f"ğŸ” è¿‡æ»¤é¡¹ç›®: {args.project}")
        conversations = [c for c in conversations if args.project.lower() in c['project'].lower()]

        if not conversations:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°é¡¹ç›® '{args.project}' çš„å¯¹è¯è®°å½•")
            sys.exit(1)

    print(f"âœ… æ‰¾åˆ° {len(conversations)} æ¡å¯¹è¯")
    print(f"ğŸ“ æ­£åœ¨ç”Ÿæˆæ—¥è®°...")

    # Generate markdown
    markdown_content = generate_markdown_diary(conversations, args.date)

    if not markdown_content:
        print("âŒ ç”Ÿæˆæ—¥è®°å¤±è´¥")
        sys.exit(1)

    # Print to terminal if requested
    if args.print:
        print("\n" + "="*80)
        print(markdown_content)
        print("="*80 + "\n")

    # Save to file
    if args.output:
        # Custom output path
        output_path = Path(args.output)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"âœ… æ—¥è®°å·²ä¿å­˜åˆ°: {output_path}")
    else:
        # Default save location
        save_diary(markdown_content, args.date)

    print("\nğŸ‰ å®Œæˆï¼")


if __name__ == '__main__':
    main()
